{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "b53470d6-0f20-414c-8519-18c38e659bac",
   "metadata": {},
   "outputs": [],
   "source": [
    "from functools import partial\n",
    "from typing import Any, Mapping, Sequence\n",
    "\n",
    "import distrax\n",
    "import haiku as hk\n",
    "import jax\n",
    "import jax.numpy as jnp\n",
    "import jax.scipy.stats as stats\n",
    "import numpy as np\n",
    "import optax\n",
    "from jax import random\n",
    "\n",
    "Array = jnp.ndarray\n",
    "PRNGKey = Array\n",
    "Batch = Mapping[str, np.ndarray]\n",
    "OptState = Any"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "d0212c55-45e2-4b03-9a9e-a5f67468ff4a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def diag_gaussian_sample(rng, mean, log_std):\n",
    "    # Take a single sample from a diagonal multivariate Gaussian.\n",
    "    return mean + jnp.exp(log_std) * random.normal(rng, mean.shape)\n",
    "\n",
    "\n",
    "def diag_gaussian_logpdf(x, mean, log_std):\n",
    "    # Evaluate a single point on a diagonal multivariate Gaussian.\n",
    "    return jnp.sum(jax.vmap(stats.norm.logpdf)(x, mean, jnp.exp(log_std)))\n",
    "\n",
    "\n",
    "def elbo(logprob, rng, mean, log_std):\n",
    "    # Single-sample Monte Carlo estimate of the variational lower bound.\n",
    "    sample = diag_gaussian_sample(rng, mean, log_std)\n",
    "    return logprob(sample) - diag_gaussian_logpdf(sample, mean, log_std)\n",
    "\n",
    "\n",
    "def batch_elbo(logprob, rng, params, num_samples):\n",
    "    # Average over a batch of random samples.\n",
    "    rngs = random.split(rng, num_samples)\n",
    "    vectorized_elbo = jax.vmap(partial(elbo, logprob), in_axes=(0, None, None))\n",
    "    return jnp.mean(vectorized_elbo(rngs, *params))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "dcc5d630-53c5-43a8-b12a-6adcc3299773",
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_conditioner(\n",
    "    event_shape: Sequence[int], hidden_sizes: Sequence[int], num_bijector_params: int\n",
    ") -> hk.Sequential:\n",
    "    \"\"\"Creates an MLP conditioner for each layer of the flow.\"\"\"\n",
    "    return hk.Sequential(\n",
    "        [\n",
    "            hk.Flatten(preserve_dims=-len(event_shape)),\n",
    "            hk.nets.MLP(hidden_sizes, activate_final=True),\n",
    "            # We initialize this linear layer to zero so that the flow is initialized\n",
    "            # to the identity function.\n",
    "            hk.Linear(\n",
    "                np.prod(event_shape) * num_bijector_params,\n",
    "                w_init=jnp.zeros,\n",
    "                b_init=jnp.zeros,\n",
    "            ),\n",
    "            hk.Reshape(tuple(event_shape) + (num_bijector_params,), preserve_dims=-1),\n",
    "        ]\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "9fa8efb6-0b1a-4562-96ef-820a0edc62fa",
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_flow_model(\n",
    "    event_shape: Sequence[int],\n",
    "    num_layers: int,\n",
    "    hidden_sizes: Sequence[int],\n",
    "    num_bins: int,\n",
    ") -> distrax.Transformed:\n",
    "    \"\"\"Creates the flow model.\"\"\"\n",
    "    # Alternating binary mask.\n",
    "    mask = jnp.arange(0, np.prod(event_shape)) % 2\n",
    "    mask = jnp.reshape(mask, event_shape)\n",
    "    mask = mask.astype(bool)\n",
    "\n",
    "    def bijector_fn(params: Array):\n",
    "        return distrax.RationalQuadraticSpline(params, range_min=0.0, range_max=1.0)\n",
    "\n",
    "    # Number of parameters for the rational-quadratic spline:\n",
    "    # - `num_bins` bin widths\n",
    "    # - `num_bins` bin heights\n",
    "    # - `num_bins + 1` knot slopes\n",
    "    # for a total of `3 * num_bins + 1` parameters.\n",
    "    num_bijector_params = 3 * num_bins + 1\n",
    "\n",
    "    layers = []\n",
    "    for _ in range(num_layers):\n",
    "        layer = distrax.MaskedCoupling(\n",
    "            mask=mask,\n",
    "            bijector=bijector_fn,\n",
    "            conditioner=make_conditioner(\n",
    "                event_shape, hidden_sizes, num_bijector_params\n",
    "            ),\n",
    "        )\n",
    "        layers.append(layer)\n",
    "        # Flip the mask after each layer.\n",
    "        mask = jnp.logical_not(mask)\n",
    "\n",
    "    # We invert the flow so that the `forward` method is called with `log_prob`.\n",
    "    flow = distrax.Inverse(distrax.Chain(layers))\n",
    "    base_distribution = distrax.Independent(\n",
    "        distrax.Uniform(low=jnp.zeros(event_shape), high=jnp.ones(event_shape)),\n",
    "        reinterpreted_batch_ndims=len(event_shape),\n",
    "    )\n",
    "\n",
    "    return distrax.Transformed(base_distribution, flow)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4808d651-2110-4436-89a1-93c53446b6c8",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3398c9d8-09e7-4625-8129-960bd9b76f73",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = make_flow_model("
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
